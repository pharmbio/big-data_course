{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a QSAR model using the Morgan fingerprint in rdkit and Random forest from Spark's Machine Learning\n",
    "\n",
    "## QSAR\n",
    "We know that the structure of a molecule determines the molecule's properties, but we are not capable of calculating molecular properties from molecular structures in general. One approach to doing this is used in the field of Quantitative Structure-Activity Relationships (QSAR) / Quantitative Structure-Property Relationship (QSPR), where a molecular structure is mathematically described and then machine learning is used to predict molecular activity or molecular properties. The names QSAR and QSPR are sometimes used a bit sloppily and mixed up, QSAR seems to be the most common term but QSPR is arguably a bit more general. So we need a strategy for describing the molecular properties, a machine larning algorithm, and a lot of data that we can use to train our machine larning algorithm on.\n",
    "\n",
    "### Molecular descriptor\n",
    "A molecular descriptor is a mathematical representation of a molecule resulting from the transformation of the symbolic representation of a molecule into numbers. One approach might simply be to count the number of different atoms or fragments. There are many different approaches, some calculated from the matrix resulting from the pairwise distances between all atoms in the molecule, some being fragment based. The idea is to create a vector of numbers calculated in such a way that similar compounds will have similar vectors. There are two different approaches worth mentioning. Either each posiition in the vector is predefined and map to one thing, meaning that we know exactly what each position means, (_e.g._, if a position in the vector has the number 1 we know that the molecule contains exactly the substructure that corresponds to that position), or a _hashing_ algorithm is used to go from a descriptor to a position in the vector. The hashing algorithm is a one-way algorithm that turns, _e.g._, a string into a number (corresponding to the position in the vector). The hashing approach has the benefit that it does not require someone to predefine which structures should be used, anything found can be hashed and used in the vector but we can not easily know what each position means, and there is the risk for hash collisions (_i.e._, the hashing algorithm mapping many things into the same number making it impossible to say which one it originally was). In this lab we will use a circular fingerpint known as the Morgan fingerprint as implemented in the Python library RDKit and hash it down to a vector. A circular fingerprint uses each atom in a molecular and describe the atom neighbours out to a certain distance or \"radius\".\n",
    "\n",
    "### Machine learning algorithm\n",
    "We will use the Random Forest algorithm, which is a good algorithm to start with. We will not go into details on how the algorithm works in this lab, but it constructs a multitude of decission trees and then weights them together.\n",
    "\n",
    "### Dataset\n",
    "The data we will look at in this lab is distribution coefficient, log D, we will use a calculated value that we have extracted from a database where this number has been calculated for many substances. Since we want to work with big data we will use a calculated value. Of course the quality of the model depends heavily on the number of training examples.\n",
    "\n",
    "Let's start by looking at the first lines of our datafile:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a tab separated file where the first line contains names for the columns and then follows SMILES and log D for all substances, one substance per line.\n",
    "\n",
    "For dealing with the chemistry we will use [RDKit](http://www.rdkit.org/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import DataStructs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to import the `SparkSession` and for the machine learning we will use `pyspark.ml`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will use a `numpy` array to convert from the RDKit representation of our dataset to the PySPARK representation of our dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to get a `SparkSession` which will work as our connection to the Spark cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"spark://192.168.2.32:7077\") \\\n",
    "        .appName(\"RF_on_MorganFP\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "        .config(\"spark.shuffle.service.enabled\", True)\\\n",
    "        .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"30s\")\\\n",
    "        .config(\"spark.executor.cores\",2)\\\n",
    "        .config(\"spark.driver.port\",9998)\\\n",
    "        .config(\"spark.blockManager.port\",10005)\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have placed the dataset file on the Hadoop file system accessible for our script at: `hdfs://192.168.2.32:9000/acd_logd.smiles`. The dataset file starts with a header line and has the data in tab separated columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\",\"true\")\\\n",
    "               .option(\"delimiter\", '\\t').csv(\"hdfs://192.168.2.32:9000/acd_logd.smiles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a couple of rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And count the number of lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is quite many mulecular structures in this dataset and since we don't have so much time we will sample a fraction of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraction = 0.01\n",
    "\n",
    "df_sample = df.sample(fraction)\n",
    "df_sample.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create the input the to machine learning algorithm. In the end we want to have a dataframe with the logD values and the calculated molecular descriptor. The descriptor will consist of 2048 columns. In order to run RDKit in the Spark workers we will use the RDD representation and map it over to RDKit molecules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chemicals = df_sample.select(\"canonical_smiles\", \"acd_logd\").rdd\\\n",
    "                     .map( lambda row: (row.canonical_smiles, float(row.acd_logd)) )\\\n",
    "                     .map( lambda x: (Chem.MolFromSmiles(x[0]), x[1]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we are hauling around the logd value in a tuple since we need to have it in the final step and since all data is spread out in the cluster we can not easily just add a column. Next we calculate the Morgan fingerprint for the molecules using 1024 bits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fingerprints = chemicals.map( lambda x: (AllChem.GetMorganFingerprintAsBitVect(x[0], 2, nBits=1024), x[1]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I already mentioned, we will use a Numpy `array` in order to convert the fingerprints (tupled with the logD values) in a dense PySPARK vector and then to a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = fingerprints.map( lambda x: (np.array(x[0]),x[1]) )\\\n",
    "                   .map( lambda x: (Vectors.dense(x[0].tolist()),x[1]) )\\\n",
    "                   .toDF([\"features\", \"label\"])\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will run random forest in Spark. We will specify how to index the features while automatically identifying categorical features by saying anything more than 4 distinct values as continous (our fingperints ahve two values, 0 or 1). We will also split the data into training and test set and specify a random forest model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n",
    "\n",
    "# 30% for test set\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "rf = RandomForestRegressor(featuresCol=\"indexedFeatures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will chain the `featureIndexer` and the `RandomForestRegressor` using a `pyspark.ml.Pipeline` and train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[featureIndexer, rf])\n",
    "model = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in order to say something about how good our model is we will predict log D for the molecules in our test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(testData)\n",
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we will calculate Root Mean Squared Error (RMSE):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and stop our Spark session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, if you have the time you could try to sample a larger part of the dataset and see if you can get a better RMSE, truth be told this one is maybe not so impressive..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
